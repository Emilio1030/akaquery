{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57226901",
   "metadata": {},
   "source": [
    "# Converting the PDF files into vector database  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f7c8d88",
   "metadata": {},
   "source": [
    "# 1. Initial setup\n",
    "## 1.1. Imports\n",
    "This setup includes loading environment variables from a `.env` file, setting the required environment variables, and importing the necessary modules for further processing. It ensures that the code has access to the required APIs and functions for the subsequent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1440f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import chromadb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06fc759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the variables from .env file and set the API key (or user may manually set the API key)\n",
    "load_dotenv()  \n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv('ANTHROPIC_API_KEY')\n",
    "#os.environ[\"MATHPIX_API_ID\"] = os.getenv('MATHPIX_API_KEY')\n",
    "#openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Langchain framework\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel # for RAG with source\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "## The following loaders are used for options\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_community.document_loaders import MathpixPDFLoader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84652248",
   "metadata": {},
   "source": [
    "## 1.2. Initial variable setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35630ca8-a707-4445-b8a2-661fe3312d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial variable setup\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "db_directory = \"./data/chroma_semantic\"\n",
    "USE_Anthropic = True\n",
    "\n",
    "if USE_Anthropic:\n",
    "    llm = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\", temperature=0)\n",
    "else:\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) # context window size 16k for GPT 3.5 Turbo\n",
    "\n",
    "collection_list=[\n",
    "    \"ASOP_life\",\n",
    "    \"Bermuda\",\n",
    "    \"CFT\",\n",
    "    \"PBR\",\n",
    "    \"VM21\",\n",
    "    \"VM22\",\n",
    "    \"Asset\",\n",
    "    \"IFRS17\"\n",
    "]\n",
    "#collection_list = [\"PBR\"] # for testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b04673e4",
   "metadata": {},
   "source": [
    "# 2. Load PDF files and convert to a vector DB\n",
    "## 2.1. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and extract text from PDFs in a folder\n",
    "def get_file_name(source_path):\n",
    "    return source_path.split('/')[-1]\n",
    "\n",
    "def load_pdfs_from_folder(folder_path, loader_option):\n",
    "    # Get a list of PDF files in the specified folder\n",
    "    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "    docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        file_name = get_file_name(pdf_file)\n",
    "        \n",
    "        if loader_option == 1:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file)\n",
    "        elif loader_option == 2:\n",
    "            # PyPDFium2Loader is known to be faster than PyPDFLoader\n",
    "            loader = PyPDFium2Loader(pdf_file)\n",
    "        elif loader_option == 3:\n",
    "            # PyMuPDFLoader is known to be general purpose, rich metadata\n",
    "            loader = PyMuPDFLoader(pdf_file)\n",
    "        elif loader_option == 4:\n",
    "            # Allows automated concatenate pages\n",
    "            loader = PDFMinerLoader(pdf_file, concatenate_pages=True)\n",
    "        \n",
    "        loaded_docs = loader.load()\n",
    "        \n",
    "        for doc in loaded_docs:\n",
    "            doc.metadata['source'] = file_name\n",
    "        \n",
    "        docs.extend(loaded_docs)\n",
    "    return docs\n",
    "\n",
    "def pdf_to_md(folder_path, download_path, loader_option):\n",
    "    # Get a list of PDF files in the specified folder\n",
    "    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
    "    for pdf_file in pdf_files:\n",
    "        file_name = get_file_name(pdf_file)\n",
    "        base_name = file_name.replace('.pdf', '')\n",
    "        \n",
    "        if loader_option == 1:\n",
    "            # Load the PDF file using the PyPDFLoader\n",
    "            loader = PyPDFLoader(pdf_file)\n",
    "        elif loader_option == 2:\n",
    "            # PyPDFium2Loader is known to be faster than PyPDFLoader\n",
    "            loader = PyPDFium2Loader(pdf_file)\n",
    "        elif loader_option == 3:\n",
    "            # PyMuPDFLoader is known to be general purpose, rich metadata\n",
    "            loader = PyMuPDFLoader(pdf_file)\n",
    "        elif loader_option == 4:\n",
    "            # Allows automated concatenate pages\n",
    "            loader = PDFMinerLoader(pdf_file, concatenate_pages=True)\n",
    "        elif loader_option == 5:\n",
    "            # Use Mathpix OCR to load formula, tables\n",
    "            # may be slower, but higher quality than all above\n",
    "            # Require Mathpix API ID - 3 cents per pdf page\n",
    "            loader = MathpixPDFLoader(pdf_file)\n",
    "        \n",
    "        loaded_docs = loader.load()\n",
    "        \n",
    "        for i, doc in enumerate(loaded_docs):\n",
    "            doc.metadata['source'] = file_name\n",
    "            if loader_option > 3:\n",
    "                md_file_name = f\"{download_path}/{base_name}.md\"\n",
    "            else:\n",
    "                md_file_name = f\"{download_path}/{base_name}{i+1:03d}.md\"\n",
    "            with open(md_file_name, 'w', encoding='utf-8') as md_file:\n",
    "                md_file.write(doc.page_content)\n",
    "\n",
    "def load_mds_from_folder(folder_path):\n",
    "    # Get a list of md files in the specified folder\n",
    "    md_files = glob.glob(f\"{folder_path}/*.md\")\n",
    "    docs = []\n",
    "    for md_file in md_files:\n",
    "        file_name = get_file_name(md_file)\n",
    "        base_name = file_name.replace('.md', '')\n",
    "        pdf_file_name = f\"{base_name}.pdf\"\n",
    "        \n",
    "        loader = UnstructuredMarkdownLoader(md_file)\n",
    "        loaded_docs = loader.load()\n",
    "        \n",
    "        for doc in loaded_docs:\n",
    "            doc.metadata['source'] = pdf_file_name\n",
    "        \n",
    "        docs.extend(loaded_docs)\n",
    "    return docs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be7ad53a",
   "metadata": {},
   "source": [
    "## 2.2. Convert PDFs to markdown files and then convert to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb282c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Run only to convert pdf to markdown files\n",
    "############################################################################\n",
    "\n",
    "collection_list=[\n",
    "    #\"Cayman\",\n",
    "    # \"AI_BigData\",\n",
    "    #\"ASOP_life\",\n",
    "    \"Bermuda\",\n",
    "    # \"CFT\",\n",
    "    # \"GAAP\",\n",
    "    # \"RiskFinance\",\n",
    "    # \"PBR\",\n",
    "    # \"VM21\",\n",
    "    # \"VM22\",\n",
    "    # \"Asset\",\n",
    "    # \"IFRS17\",\n",
    "]\n",
    "for collection_name in collection_list:\n",
    "    # Put new files in the upload subfolder\n",
    "    folder_path = './data/upload/pdf/'+collection_name\n",
    "    download_path = './data/upload/md/'+collection_name\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "    # Use loader option 5 to use Mathpix OCR to load formula, tables\n",
    "    pdf_to_md(folder_path, download_path, loader_option = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Run to load markdown files to vector database\n",
    "############################################################################\n",
    "collection_list=[\n",
    "    #\"Cayman\",\n",
    "    # \"AI_BigData\",\n",
    "    #\"ASOP_life\",\n",
    "    \"Bermuda\",\n",
    "    # \"CFT\",\n",
    "    # \"GAAP\",\n",
    "    # \"RiskFinance\",\n",
    "    # \"PBR\",\n",
    "    # \"VM21\",\n",
    "    # \"VM22\",\n",
    "    # \"Asset\",\n",
    "    # \"IFRS17\",\n",
    "]\n",
    "\n",
    "for collection_name in collection_list:\n",
    "    # Put new files in the upload subfolder\n",
    "    folder_path = './data/upload/md/'+collection_name\n",
    "\n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_mds_from_folder(folder_path)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=1000, # 1000 splits a page into roughly 3 chunks\n",
    "    #     chunk_overlap=200,\n",
    "    #     length_function=len,)\n",
    "\n",
    "    # Use semantic chunker to increase meaningfulness of the chunks\n",
    "    text_splitter = SemanticChunker(embeddings_model)\n",
    "\n",
    "    # Split the documents into chunks using the text splitter\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create a Chroma vector database from the document splits\n",
    "    Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=embeddings_model, \n",
    "        persist_directory=db_directory,\n",
    "        collection_name=collection_name,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a41b27f5",
   "metadata": {},
   "source": [
    "## 2.3. (NOT USED) Convert PDFs to vector database directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Run to load pdf files to vector database\n",
    "############################################################################\n",
    "\n",
    "for collection_name in collection_list:\n",
    "    # Put new files in the upload subfolder\n",
    "    folder_path = './data/upload/pdf/'+collection_name\n",
    "\n",
    "    # Call the function to load and extract text from PDFs in the specified folder\n",
    "    docs = load_pdfs_from_folder(folder_path, loader_option = 1)\n",
    "    \n",
    "    # Create a text splitter object with specified parameters\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size=1000, # 1000 splits a page into roughly 3 chunks\n",
    "    #     chunk_overlap=200,\n",
    "    #     length_function=len,)\n",
    "\n",
    "    # Use semantic chunker to increase meaningfulness of the chunks\n",
    "    text_splitter = SemanticChunker(embeddings_model)\n",
    "\n",
    "    # Split the documents into chunks using the text splitter\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create a Chroma vector database from the document splits\n",
    "    Chroma.from_documents(\n",
    "        documents=splits, \n",
    "        embedding=embeddings_model, \n",
    "        persist_directory=db_directory,\n",
    "        collection_name=collection_name,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d46105ba",
   "metadata": {},
   "source": [
    "# 3. For test purposes\n",
    "## 3.1. Define vector store from vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a user may choose different collection name from the list\n",
    "# [\"ASOP_life\", \"Bermuda\", \"CFT\", \"VM21\", \"VM22\", \"Asset\", \"IFRS17\"]\n",
    "collection_name = collection_list[0] \n",
    "\n",
    "# Get a Chroma vector database with specified parameters\n",
    "vectorstore = Chroma(embedding_function=embeddings_model, \n",
    "                     persist_directory=db_directory,\n",
    "                     collection_name=collection_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90a4d0e8",
   "metadata": {},
   "source": [
    "## 3.2. Retrieve from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve and RAG chain\n",
    "# Create a retriever using the vector database as the search source\n",
    "# You may choose a specific document to filter the search\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", \n",
    "                                     search_kwargs={\n",
    "                                        'k': 6, \n",
    "                                        'lambda_mult': 0.5,\n",
    "                                        # 'filter': {'source': '201611-Guidance-Notes-for-Commercial-Insurers-and-Groups-Statutory-Reporting-Regime-30-Nov-2016.pdf'}\n",
    "                                        }\n",
    "                                    ) \n",
    "# Use MMR (Maximum Marginal Relevance) to find a set of documents that are both similar to the input query and diverse among themselves\n",
    "# Increase the number of documents to get, and increase diversity (lambda mult 0.5 being default, 0 being the most diverse, 1 being the least)\n",
    "\n",
    "# Load the RAG (Retrieval-Augmented Generation) prompt\n",
    "qa_system_prompt = \"\"\"You are a helpful assistant to help actuaries with question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "ASOP or asop means Actuarial Standards of Practice. \\\n",
    "CFT means Cash Flow Testing. AAT means Asset Adequacy Testing. \\\n",
    "BMA means Bermuda Monetary Authority. \\\n",
    "SBA means scenario-based approach. BEL means best estimate liabilities.\\\n",
    "After you answer, provide the sources you used to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "\n",
    "{context}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a function to format the documents with their sources and pages\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    #sources_pages = \"\\n\".join(f\"{doc.metadata['source']} (Page {doc.metadata['page'] + 1})\" for doc in docs)\n",
    "    sources_pages = \"\\n\".join(f\"{doc.metadata['source']})\" for doc in docs)\n",
    "    # Added 1 to the page number assuming 'page' starts at 0 and we want to present it in a user-friendly way\n",
    "\n",
    "    return f\"Documents:\\n{formatted_docs}\\n\\nSources and Pages:\\n{sources_pages}\"\n",
    "\n",
    "# Create a RAG chain using the formatted documents as the context\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_sources(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel chain for retrieving and generating answers\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9243e34a",
   "metadata": {},
   "source": [
    "## 3.3. Generate Q&A Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceedb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    # Prompt the user for a question on ASOP\n",
    "    usr_input = input(\"What is your question on ASOP?: \")\n",
    "\n",
    "    # Invoke the RAG chain with the user input as the question\n",
    "    output = rag_chain_with_source.invoke(usr_input)\n",
    "\n",
    "    # Generate the Markdown output with the question, answer, and context\n",
    "    markdown_output = \"### Question\\n{}\\n\\n### Answer\\n{}\\n\\n### Context\\n\".format(output['question'], output['answer'])\n",
    "\n",
    "    last_page_content = None  # Variable to store the last page content\n",
    "    i = 1 # Source indicator\n",
    "\n",
    "    # Iterate over the context documents to format and include them in the output\n",
    "    for doc in output['context']:\n",
    "        current_page_content = doc.page_content.replace('\\n', '  \\n')  # Get the current page content\n",
    "        \n",
    "        # Check if the current content is different from the last one\n",
    "        if current_page_content != last_page_content:\n",
    "            #markdown_output += \"- **Source {}**: {}, page {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], doc.metadata['page'], current_page_content)\n",
    "            markdown_output += \"- **Source {}**: {}:\\n\\n{}\\n\".format(i, doc.metadata['source'], current_page_content)\n",
    "            i = i + 1\n",
    "        last_page_content = current_page_content  # Update the last page content\n",
    "    \n",
    "    # Display the Markdown output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d54daa6",
   "metadata": {},
   "source": [
    "### Example questions related to ASOPs\n",
    "- explain ASOP No. 14\n",
    "- How are expenses relfected in cash flow testing based on ASOP No. 22?\n",
    "- What is catastrophe risk?\n",
    "- When do I update assumptions?\n",
    "- What should I do when I do not have credible data to develop non-economic assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36183436",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5938771",
   "metadata": {},
   "source": [
    "# 4. References\n",
    "- https://www.actuarialstandardsboard.org/standards-of-practice/\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/sources\n",
    "- https://python.langchain.com/docs/integrations/text_embedding/\n",
    "- https://python.langchain.com/docs/integrations/vectorstores/chroma\n",
    "- https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All\n",
    "- https://chat.langchain.com/\n",
    "- https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.chroma.Chroma.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "628913a4",
   "metadata": {},
   "source": [
    "# 5. Management of the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=db_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e514b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.delete_collection(name=\"VM20\") # Delete a collection and all associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdeddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(name=\"AI_BigData\") \n",
    "collection.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5025ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.modify(name=\"PBR\") rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Get all the metadatas\n",
    "metadatas = collection.get()['metadatas']\n",
    "\n",
    "# Create a dictionary to store distinct source names\n",
    "distinct_sources = defaultdict(set)\n",
    "\n",
    "# Iterate through metadatas and add source names to the dictionary\n",
    "for metadata in metadatas:\n",
    "    source = metadata.get('source', None)\n",
    "    if source:\n",
    "        distinct_sources[source].add(source)\n",
    "\n",
    "# Get the distinct source names as a list\n",
    "distinct_source_names = list(distinct_sources.keys())\n",
    "\n",
    "print(distinct_source_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    where={\"source\": \"2023_BMA-Supervision-and-Regulation-of-Private-Equity-Insurers.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.delete(\n",
    "    #ids=[\"id1\", \"id2\", \"id3\",...],\n",
    "    where={\"source\": \"VA_PN_Supplement_Exposure_Draft.pdf\"}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f66a6a9c",
   "metadata": {},
   "source": [
    "# Rename process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1410cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename process\n",
    "folder_name = \"VM22\"\n",
    "old_file_name = \"VM-22 Subgroup Draft July 2023 Clean.pdf\"\n",
    "new_file_name = \"202307-NAIC-VM-22 Subgroup Draft.pdf\"\n",
    "\n",
    "# Chroma DB update\n",
    "collection = client.get_collection(name=folder_name)\n",
    "results = collection.get(where={\"source\": old_file_name})\n",
    "ids_to_update = results['ids']\n",
    "print(ids_to_update)\n",
    "new_metadatas = [{\"source\": new_file_name} for _ in ids_to_update]\n",
    "collection.update(ids=ids_to_update, metadatas=new_metadatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c56113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# json file summary update\n",
    "with open('summary.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Update the existing data with the new key-value pair\n",
    "data[new_file_name] = data.pop(old_file_name)\n",
    "\n",
    "with open('summary.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# md and pdf file update\n",
    "# Construct the paths to the folders\n",
    "md_folder = os.path.join(\"data/md\", folder_name)\n",
    "pdf_folder = os.path.join(\"data/pdf\", folder_name)\n",
    "\n",
    "# Construct the old and new file paths\n",
    "old_file_name_md = old_file_name.replace(\".pdf\", \".md\")\n",
    "new_file_name_md = new_file_name.replace(\".pdf\", \".md\")\n",
    "old_md_path = os.path.join(md_folder, old_file_name_md)\n",
    "new_md_path = os.path.join(md_folder, new_file_name_md)\n",
    "\n",
    "# Check if the old MD file exists before renaming\n",
    "if os.path.exists(old_md_path):\n",
    "    # Rename the MD file\n",
    "    os.rename(old_md_path, new_md_path)\n",
    "else:\n",
    "    print(f\"Skipping MD file rename: {old_md_path} does not exist.\")\n",
    "\n",
    "# Construct the old and new file paths\n",
    "old_pdf_path = os.path.join(pdf_folder, old_file_name)\n",
    "new_pdf_path = os.path.join(pdf_folder, new_file_name)\n",
    "\n",
    "# Rename the file\n",
    "os.rename(old_pdf_path, new_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92954856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ValAct_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
